# Anomaly detection

This guide explains how station features are used to detect unexpected network conditions using an online Isolation Forest model.

## Motivation

Timely identification of disruptions is critical for passenger information systems. We monitor per‑station snapshots and flag anomalies when the feature vectors deviate from historic patterns.

## Available features

The detector operates on the snapshot features generated by
[`SnapshotFeatureBuilder`](feature_engineering.md). These columns are:

- `arrival_delay_t`, `departure_delay_t`
- `headway_t`, `rel_headway_t`, `dwell_delta_t`
- `delay_arrival_grad_t`, `delay_departure_grad_t`
- `upstream_delay_mean_2`, `downstream_delay_max_2`
- rolling statistics: `delay_mean_5`, `delay_std_5`, `delay_mean_15`, `headway_p90_60`
- time features: `sin_hour`, `cos_hour`, `day_type`
- network metrics: `node_degree`, `hub_flag`, `central_flag`
- vehicle metrics: `congestion_level`, `occupancy_status`
- presence indicators: `is_train_present`, `data_fresh_secs`
  (a `route_id` column is present only when multiple routes appear in a snapshot)

`StreamingIForestDetector` automatically considers every numeric column except
`snapshot_timestamp`, `stop_id`, `direction_id`, and `local_dt`.

## Preprocessing

The raw feature files are sanitised before scoring:

1. missing values are filled with `0`
2. only snapshots for the designated station IDs are kept
3. snapshots where all numeric features are `0` are dropped
4. scaling is currently disabled (`MinMaxScaler` placeholder is commented out)
5. features `dwell_delta_t` and `data_fresh_secs` are dropped
6. the analysis is limited to the following station IDs:
   `2155269, 2155267, 2155265, 2153402, 2153404, 2154264, 2154262,
   2126159, 2121225, 2113351, 2113341, 2113361, 2067142, 2065163,
   2060115, 2000460, 2000463, 2000464, 2000467, 2017078, 204471`

## Algorithm

A **River** `IsolationForest` is wrapped in a sliding window to maintain a fixed number of past scores. The detector skips alerting during the warm‑up period (`warmup_days = 4`). Results are written to

```text
data/anomaly_scores/year=YYYY/month=MM/day=DD/anomaly_scores_YYYY-DD-MM-HH-MM.parquet
```

## Hyper‑parameters

| name | range |
| --- | --- |
| `n_trees` | 50 – 100 |
| `height` | 8 – 10 |
| `subsample_size` | 128 – 256 |
| `window_size` | 5 000 – 10 000 |
| `threshold_quantile` | 0.99 |
| `warmup_days` | fixed 4 |

## Evaluation metrics

We evaluate using these metrics:

- lead‑time ROC‑AUC – primary score
- precision@k – secondary measure
- mean time to detection (MTTD)
- false positive rate (FPR)

## Tuning procedure

A grid of 16 parameter combinations is explored serially. Scores are cached to avoid re‑reading identical snapshots. The outputs are `tuning_results.csv` and the best configuration in `iforest_best.yaml`.

## Notes on data

Temporary stations are excluded from training and evaluation to avoid short‑term construction noise.

## Example: scoring a time range

Feature snapshots are saved under `data/stations_features_time_series` in day-based partitions:

```text
data/stations_features_time_series/year=YYYY/month=MM/day=DD/stations_feats_YYYY-DD-MM-HH-MM.parquet
```

The snippet below streams two hours of feature files and returns anomaly scores.

```python
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
from metro_disruptions_intelligence.detect.streaming_iforest import StreamingIForestDetector

processed_root = Path("data/stations_features_time_series")
start = datetime(2023, 5, 1, 0, 0)
end = start + timedelta(hours=2)
rows = []
det = StreamingIForestDetector("configs/iforest_default.yaml")
for ts in range(int(start.timestamp()), int(end.timestamp()), 60):
    dt = datetime.fromtimestamp(ts)
    f = (
        processed_root / f"year={dt.year:04d}" / f"month={dt.month:02d}" /
        f"day={dt.day:02d}" / f"stations_feats_{dt:%Y-%d-%m-%H-%M}.parquet"
    )
    if not f.exists():
        continue
    df = pd.read_parquet(f)
rows.append(det.score_and_update(df, explain=True))

scores = pd.concat(rows, ignore_index=True)

```

## Evaluating alerts

The `evaluation` helpers compute detection metrics against the alerts feed.

```python
from metro_disruptions_intelligence.processed_reader import load_rt_dataset
from metro_disruptions_intelligence.evaluation import build_events, evaluate_scores

alerts = load_rt_dataset(Path("data/processed/rt"), feeds=["alerts"])
events = build_events(alerts)
metrics = evaluate_scores(scores, events)

```

The resulting dictionary contains the ROC‑AUC, precision@k, mean time to detection and false positive rate.
